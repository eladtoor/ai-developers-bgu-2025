"""
RAGAS Evaluation Script for Timeline Extraction System

This script evaluates the timeline extraction system using RAGAS metrics
with ground truth data from the house break-in story.
"""

import json
import pandas as pd
from dotenv import load_dotenv
load_dotenv()  # Load environment variables from .env file
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    answer_correctness,
    answer_similarity,
    context_precision,
    context_entity_recall
)
from datasets import Dataset
import os

def load_ground_truth(ground_truth_file="ground_truth_dataset.json"):
    """Load ground truth data from JSON file."""
    try:
        with open(ground_truth_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"Ground truth file {ground_truth_file} not found!")

def create_evaluation_dataset(ground_truth_data, model_answers):
    """
    Create a RAGAS-compatible dataset for evaluation.
    
    Args:
        ground_truth_data: The loaded ground truth data
        model_answers: List of answers generated by your timeline system
        
    Returns:
        Dataset: RAGAS-compatible dataset
    """
    if len(model_answers) != len(ground_truth_data["ground_truth"]):
        raise ValueError(f"Number of model answers ({len(model_answers)}) must match number of ground truth questions ({len(ground_truth_data['ground_truth'])})")
    
    # Extract data from ground truth
    questions = [item["question"] for item in ground_truth_data["ground_truth"]]
    contexts = [item["context"] for item in ground_truth_data["ground_truth"]]
    ground_truth_answers = [item["answer"] for item in ground_truth_data["ground_truth"]]
    
    # Create dataset
    dataset_dict = {
        "question": questions,
        "contexts": [[context] for context in contexts],  # RAGAS expects list of contexts
        "answer": model_answers,
        "ground_truth": ground_truth_answers
    }
    
    return Dataset.from_dict(dataset_dict)

def run_ragas_evaluation(model_answers, output_file="evaluation_results.json"):
    """
    Evaluate the timeline system using RAGAS metrics.
    
    Args:
        model_answers: List of answers generated by your timeline system
        output_file: File to save evaluation results
        
    Returns:
        Dict containing evaluation results
    """
    print("Loading ground truth data...")
    ground_truth_data = load_ground_truth()
    
    print("Creating evaluation dataset...")
    dataset = create_evaluation_dataset(ground_truth_data, model_answers)
    
    print("Running RAGAS evaluation...")
    results = evaluate(
        dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_recall,
            answer_correctness,
            answer_similarity,
            context_precision,
            context_entity_recall
        ]
    )
    
    # Convert results to dictionary
    results_dict = {
        "metrics": results.scores,
        "metadata": {
            "total_questions": len(model_answers),
            "ground_truth_source": ground_truth_data["metadata"]["source_document"],
            "categories": ground_truth_data["metadata"]["categories"]
        }
    }
    
    # Save results
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2)
    
    print(f"Evaluation results saved to {output_file}")
    return results_dict

def print_evaluation_summary(results):
    """Print a summary of evaluation results."""
    print("\n" + "="*50)
    print("RAGAS EVALUATION SUMMARY")
    print("="*50)
    
    metrics = results["metrics"]
    metadata = results["metadata"]
    
    print(f"\nDataset: {metadata['ground_truth_source']}")
    print(f"Total Questions: {metadata['total_questions']}")
    print(f"Categories: {metadata['categories']}")
    
    print("\nMetrics:")
    print("-" * 30)
    
    # Calculate average scores for each metric
    metric_names = [
        "faithfulness",
        "answer_relevancy", 
        "context_recall",
        "answer_correctness",
        "semantic_similarity",
        "context_precision",
        "context_entity_recall"
    ]
    
    for metric_name in metric_names:
        scores = [item[metric_name] for item in metrics]
        avg_score = sum(scores) / len(scores)
        print(f"{metric_name.replace('_', ' ').title()}: {avg_score:.4f}")
    
    print("\n" + "="*50)

def create_sample_answers_for_testing():
    """
    Create sample answers for testing the evaluation system.
    These would normally come from your timeline extraction system.
    """
    # Sample answers that simulate what your timeline system might generate
    sample_answers = [
        "8:15 PM",  # What time did Sarah first hear suspicious sounds?
        "9:00 PM",  # When did Sarah call 911?
        "9:05 PM",  # What time did the intruder enter the house?
        "9:22 PM",  # When did the police arrive?
        "9:18 PM",  # What time did Sarah hear the first police siren?
        "8:00 AM the next morning",  # When did Sarah call her insurance company?
        "9:00 AM the following day",  # What time did Sarah meet with the crime scene investigator?
        "2:00 PM the same day",  # When did Sarah receive the call about the suspect being identified?
        "6:00 PM that evening",  # What time did Sarah go to the police station to reclaim her belongings?
        "Two weeks after the break-in",  # When did Sarah begin experiencing PTSD symptoms?
        "Three months after the break-in",  # When did Sarah fully recover from the traumatic experience?
        "Sarah's grandmother's jewelry box, some cash from her bedroom drawer, and her laptop computer",  # What items were stolen?
        "Initially dismissed it as wind or a neighborhood cat, then continued watching TV",  # What did Sarah do when she first heard suspicious sounds?
        "Silently retreated to her bedroom, locked the door, and called 911",  # What did Sarah do when she realized someone was in her house?
        "Methodically searched through belongings, opened drawers and cabinets, and took valuables",  # What did the intruder do while in the house?
        "Approached the front door, announced their presence, conducted a thorough search of the house, took Sarah's statement, and documented the scene",  # What did the police do when they arrived?
        "Contacted a security company, scheduled installation of a comprehensive system including motion sensors, cameras, and monitored alarm system",  # What security measures did Sarah take?
        "The suspect was arrested, some stolen belongings were recovered including the grandmother's jewelry box, and the suspect confessed to several break-ins in the area",  # What was the outcome of the police investigation?
        "Changed her perspective on home security and personal safety, made her an advocate for neighborhood watch programs, and ultimately made her stronger and more aware of her surroundings",  # What long-term effects did the break-in have on Sarah?
        "8:15 PM - Sarah heard scratching noise; 8:45 PM - Sound evolved into knocking, Sarah realized intruder present; 9:00 PM - Sarah called 911 and hid; 9:05 PM - Intruder entered house; 9:12 PM - Intruder searched house; 9:18 PM - Police sirens heard, intruder fled; 9:22 PM - Police arrived",  # What was the sequence of events?
        "8:00 AM - Called insurance company; 10:30 AM - Returned to police station for detailed statement; 2:00 PM - Received call about suspect identification; 4:00 PM - Documented stolen belongings for insurance claim"  # What were the key events the day after the break-in?
    ]
    return sample_answers

def print_matched_answers(ground_truth_data, model_answers):
    """Print the matched answers alongside ground truth for comparison."""
    print("\n" + "="*80)
    print("MATCHED ANSWERS COMPARISON")
    print("="*80)
    
    for i, (item, answer) in enumerate(zip(ground_truth_data['ground_truth'], model_answers)):
        print(f"\nQuestion {i+1}: {item['question']}")
        print(f"Expected Answer: {item['answer']}")
        print(f"Your Timeline Answer: {answer}")
        print("-" * 60)

def extract_answers_from_timeline(timeline_path, ground_truth_data):
    """
    For each ground truth question, find the most relevant line in the timeline file.
    Returns a list of answers for RAGAS evaluation.
    """
    # Read timeline lines
    with open(timeline_path, encoding='utf-8') as f:
        timeline_lines = [line.strip('• ').strip() for line in f if line.strip() and line.strip().startswith('•')]

    answers = []
    for item in ground_truth_data['ground_truth']:
        # Try to match by time or a keyword from the answer/context
        found = None
        # Try to match by time in answer (e.g., '8:15 PM')
        if any(char.isdigit() for char in item['answer']):
            for line in timeline_lines:
                if item['answer'][:5] in line or item['answer'][:4] in line:
                    found = line
                    break
        # If not found, try to match by a keyword from the answer
        if not found:
            for word in item['answer'].split():
                if len(word) > 3:
                    for line in timeline_lines:
                        if word in line:
                            found = line
                            break
                if found:
                    break
        # If still not found, try to match by a keyword from the context
        if not found:
            for word in item['context'].split():
                if len(word) > 3:
                    for line in timeline_lines:
                        if word in line:
                            found = line
                            break
                if found:
                    break
        # If still not found, just return 'Not found'
        if not found:
            found = 'Not found'
        answers.append(found)
    return answers

def main():
    print("="*60)
    print("TIMELINE SYSTEM RAGAS EVALUATION")
    print("="*60)
    print("\nChoose which timeline output to evaluate:")
    print("1. Map-Reduce Timeline")
    print("2. Refine Timeline")
    print("3. Sample Answers (default)")
    print("Type 1, 2, or 3 and press Enter.")
    print("-"*60)
    choice = input("Your choice: ").strip()

    if choice == '1':
        timeline_path = '../timeline_system/map_reduce_timeline_house_break_in_story.txt'
        print("\nEvaluating Map-Reduce Timeline...")
        ground_truth_data = load_ground_truth()
        model_answers = extract_answers_from_timeline(timeline_path, ground_truth_data)
    elif choice == '2':
        timeline_path = '../timeline_system/refine_timeline_house_break_in_story.txt'
        print("\nEvaluating Refine Timeline...")
        ground_truth_data = load_ground_truth()
        model_answers = extract_answers_from_timeline(timeline_path, ground_truth_data)
    else:
        print("\nEvaluating with Sample Answers...")
        model_answers = create_sample_answers_for_testing()

    try:
        print("Starting evaluation...")
        results = run_ragas_evaluation(model_answers)
        print_evaluation_summary(results)
        
        # Print matched answers for comparison
        print_matched_answers(ground_truth_data, model_answers)
        
        print("\n✅ Evaluation completed successfully!")
        print("Check evaluation_results.json for detailed results.")
    except Exception as e:
        print(f"\n❌ Error during evaluation: {e}")
        print("Please check your setup and try again.")

if __name__ == "__main__":
    main() 